# AI Dataset Quality Inspector

An end-to-end **AI-powered dataset quality inspection system** that helps identify
data issues such as **missing values, bias, and distribution drift**, with both
a **FastAPI backend** and a **modern animated React dashboard**.

This project is designed for **real-world ML workflows**, portfolio demonstration,
and programs like **European Summer of Code (ESoC)**.

---

## ğŸš€ What Problem Does This Solve?

In real ML systems, models often fail not because of algorithms, but because of:
- Missing or corrupted data
- Bias across sensitive groups
- Silent distribution drift between dataset versions

This tool allows you to:
- Upload datasets
- Inspect data quality automatically
- Compare reference vs current datasets
- Visualize issues via an interactive UI

---

## âœ¨ Key Features

### ğŸ” AI Data Quality Analysis
- Missing value detection (count & percentage)
- Group bias inspection
- Fairness metrics (statistical parity, disparate impact)
- Dataset drift detection (KS test, PSI, JS divergence)

### ğŸ§  Explainable & Research-Backed
- Jupyter notebook (`analysis.ipynb`) explaining:
  - Why each metric is used
  - Observations & limitations
- Clear separation between research and production code

### ğŸŒ Backend (FastAPI)
- REST API for dataset inspection
- File upload support
- Dataset comparison endpoint
- Ready for automation and integration

### ğŸ¨ Frontend (React + Animations)
- CSV upload UI
- Animated insight cards
- Visual charts for missingness
- Modern, portfolio-grade dashboard

---

## ğŸ— Project Structure

AI-DATASET-QUALITY-INSPECTOR/
â”‚
â”œâ”€â”€ api/ # FastAPI backend
â”œâ”€â”€ inspector/ # Core AI / data quality logic
â”œâ”€â”€ frontend/ # React dashboard UI
â”œâ”€â”€ notebooks/ # Exploratory analysis & validation
â”œâ”€â”€ data/ # Sample / reference datasets
â”œâ”€â”€ examples/ # Usage examples
â”œâ”€â”€ tests/ # Tests
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md


---

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/<your-username>/ai-dataset-quality-inspector.git
cd ai-dataset-quality-inspector
2ï¸âƒ£ Install Python dependencies
pip install -r requirements.txt
3ï¸âƒ£ Start the backend (FastAPI)
uvicorn api.main:app --reload
Backend will run at:

http://127.0.0.1:8000
Swagger docs:

http://127.0.0.1:8000/docs
4ï¸âƒ£ Start the frontend (UI)
cd frontend
npm install
npm run dev
Frontend will run at:

http://localhost:5173
ğŸ§ª How to Use
â–¶ï¸ Inspect a Dataset
Open the UI

Upload a CSV file

Click Analyze

View missingness insights in animated cards & charts

ğŸ”„ Compare Reference vs Current Dataset
Upload:

Reference dataset

Current dataset

Run comparison

Detect data drift and quality changes

ğŸ““ Jupyter Notebook (Research & Validation)
The notebook located at:

notebooks/analysis.ipynb
Contains:

Exploratory data analysis

Visualizations

Bias & drift reasoning

Limitations and future work

This demonstrates methodological understanding, not just code usage.

ğŸ§  AI Methods Used
Missingness analysis

Group distribution comparison

Statistical parity

Disparate impact

Kolmogorovâ€“Smirnov test

Population Stability Index (PSI)

Jensenâ€“Shannon divergence

ğŸ”’ License
This project is released under the MIT License.

ğŸŒ± Future Improvements
Bias & fairness visualizations in UI

Dataset version history

Threshold-based alerts

Integration with openML

CI-based dataset monitoring

ğŸ¤ Contributing
Contributions are welcome.
Feel free to open issues or pull requests.

